{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import librosa\n",
    "import IPython\n",
    "import random\n",
    "import math\n",
    "import librosa\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pesq import pesq\n",
    "import torch\n",
    "import  torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from Ushape_model_without_ref import CNN_BLSTM, Ushape_Backbone, Ushape_Att_Backbone, Ushape_Att_Backbone_With_Ave, Dense_Ushape_CNN_Backbone, Stoi_net, Dense_Ushape_CNN_Backbone_With_Ave, Blstm_With_Ave\n",
    "\n",
    "def create_logger(title=\"\", dir_path='.'):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    timestamp = time.time()  # 当前时间戳\n",
    "    strtime = time.strftime(\"%m-%d %H-%M-%S\", time.localtime(timestamp))\n",
    "    logfile = '%s.txt' % (title+strtime)\n",
    "    logfile_path = os.path.join(dir_path, logfile)\n",
    "    fh = logging.FileHandler(logfile_path, mode='a')\n",
    "    ch = logging.StreamHandler()\n",
    "    # formatter = logging.Formatter(\"%(asctime)s -  %(message)s\")\n",
    "    # fh.setFormatter(formatter)\n",
    "    # ch.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 2\n",
    "\n",
    "model_layer = 1 #dense: 1-774 2-1170 3-1782    cnn:1-516   # 更新：dense:1-645 2-975 3-1485 cnn:1-516 2-520 3-528 blstm:1-257\n",
    "neurons_num = 257\n",
    "\n",
    "use_dataset = 3\n",
    "rand_seed = 9987\n",
    "evalu_target = 'pesq'\n",
    "select_model = \"stoinet\" # \"cnn\" or \"dense_cnn\"or \"mosnet\" or \"stoinet\" or \"cnn_with_ave\" or \"dense_cnn_with_ave\" or \"blstm_with_ave\"\n",
    "\n",
    "if use_dataset == 1:\n",
    "    wav_path = '/data/gwx/musan/speech_librivox_slice_8s'\n",
    "    file_dir = '/data/gwx/musan/speech_librivox_slice_8s_add_noise'\n",
    "    ref_npy_path = '/data/gwx/musan/speech_librivox_slice_8s_npy'\n",
    "\n",
    "elif use_dataset == 2:\n",
    "    wav_path = '/data/gwx/musan/speech_librivox_slice_8s'\n",
    "    file_dir = '/data/gwx/musan/speech_artificial_8s_noise_dataset/'\n",
    "    ref_npy_path = '/data/gwx/musan/speech_librivox_slice_8s_npy'\n",
    "    \n",
    "elif use_dataset == 3:\n",
    "    wav_path = '/data/gwx/musan/Version_2_speech_librivox_slice_8s'\n",
    "    file_dir = '/data/gwx/musan/Version_2_speech_artificial_8s_noise_dataset'\n",
    "    ref_npy_path = '/data/gwx/musan/Version_2_speech_librivox_slice_8s_npy'\n",
    "    \n",
    "log_dir = './output_without_reference'\n",
    "\n",
    "\n",
    "if select_model is 'cnn':\n",
    "    log_sub_dir_name = 'CNN%.1d_LSTM_FC-' % model_layer + \"data_%1d\" % use_dataset\n",
    "elif select_model is 'mosnet':\n",
    "    log_sub_dir_name = \"MosNet-dataset_%1d\" % use_dataset\n",
    "elif select_model is 'stoinet':\n",
    "    log_sub_dir_name = \"StoiNet_With_Ave_Without0-dataset_%1d\" % use_dataset\n",
    "elif select_model is 'dense_cnn_with_ave':\n",
    "    log_sub_dir_name = 'Dense_CNN%.1d_LSTM_FC_With_Ave_Without0-' % model_layer + \"dataset_%1d\" % use_dataset\n",
    "elif select_model is 'blstm_with_ave':\n",
    "    log_sub_dir_name = 'BLSTM_FC_With_Ave_Without0-' + \"dataset_%1d\" % use_dataset\n",
    "\n",
    "\n",
    "log_model_save_path = os.path.join(log_dir, evalu_target, log_sub_dir_name)\n",
    "if not os.path.exists(log_model_save_path):\n",
    "    os.makedirs(log_model_save_path)\n",
    "    \n",
    "logger = create_logger('rand_seed_%d-'%rand_seed, log_model_save_path)    \n",
    "model_ckpt_save_path = os.path.join(log_model_save_path, \"best_epoch.ckpt\")\n",
    "\n",
    "class speech_Dataset(Dataset):\n",
    "    def __init__(self, file_dir, file_list, evalu_target, wav_path, ref_npy_path):\n",
    "        self.file_dir = file_dir\n",
    "        self.file_list = file_list\n",
    "        self.wav_path = wav_path\n",
    "        self.ref_npy_path = ref_npy_path\n",
    "        self.target = evalu_target\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "#         file_index = idx // 1000\n",
    "#         frame_index = idx % 1000 + 1\n",
    "#         frame_name = 'iter_%.4d' % frame_index\n",
    "#         file_data = np.load(os.path.join(self.file_dir, self.file_list[file_index]), allow_pickle=True).item()\n",
    "        file_path = self.file_list[idx]\n",
    "        file_data = np.load(file_path, allow_pickle=True).item()\n",
    "        ref_name = file_path.split('/')[-2]\n",
    "        \n",
    "        if not os.path.exists(os.path.join(self.ref_npy_path, ref_name+'.npy')):\n",
    "            ref_dict = {}\n",
    "            ref_data, _ = librosa.load(os.path.join(self.wav_path, ref_name+'.wav'), 16000)\n",
    "            ref_stft = librosa.stft(ref_data, n_fft=512, hop_length=256)\n",
    "            ref_dict[\"raw_data\"] = ref_data\n",
    "            ref_dict[\"stft_transf\"] = ref_stft\n",
    "#             print(self.ref_npy_path, ref_name)\n",
    "            np.save(os.path.join(self.ref_npy_path, ref_name), ref_dict)\n",
    "            ref_stft = np.abs(ref_stft)\n",
    "        \n",
    "        else:\n",
    "            ref_dict = np.load(os.path.join(self.ref_npy_path, ref_name+'.npy'), allow_pickle=True).item()\n",
    "            ref_data = ref_dict['raw_data']\n",
    "            ref_stft = np.abs(ref_dict[\"stft_transf\"])\n",
    "            \n",
    "\n",
    "        system_data = file_data['raw_data']\n",
    "        frame_score = file_data['frame_score']\n",
    "        \n",
    "        if self.target is 'pesq':\n",
    "            system_score = file_data['system_score']\n",
    "#             print(system_score)\n",
    "        elif self.target is 'stoi':\n",
    "            system_score = file_data['stoi_system_score']\n",
    "            \n",
    "        system_stft = np.abs(file_data['stft_transf'])\n",
    "#         system_stft = librosa.amplitude_to_db(np.abs(librosa.stft(system_data, n_fft=512, hop_length=256)), ref=np.max)\n",
    "\n",
    "                                \n",
    "        frame_score = frame_score.repeat(8, axis=0)\n",
    "        frame_score.resize(1, system_stft.shape[1])\n",
    "        frame_score = np.expand_dims(frame_score.repeat(system_stft.shape[0], axis=0), 0)\n",
    "        system_stft = np.expand_dims(system_stft, 0)\n",
    "        ref_stft = np.expand_dims(ref_stft, 0)\n",
    "        frame_feature = np.concatenate((system_stft, ref_stft), axis=0)\n",
    "        return frame_score, system_score, np.expand_dims(frame_feature[0,:,:], 0)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # cpu\n",
    "    torch.cuda.manual_seed_all(seed)  # gpu\n",
    "    torch.backends.cudnn.deterministic = True  # consistent results on the cpu and gpu\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183380\n",
      "['/data/gwx/musan/Version_2_speech_artificial_8s_noise_dataset/guassian-rand_1s/speech-slice-004884/guassian-rand_1s-0009-bg_snr_26db-snr_-10db.npy', '/data/gwx/musan/Version_2_speech_artificial_8s_noise_dataset/guassian-rand_1s/speech-slice-000097/guassian-rand_1s-0008-bg_snr_24db-snr_12db.npy']\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "\n",
    "def print_list_dir(doc, path, suffix='.npy'):\n",
    "    #遍历文件夹下面所有以某一后缀结尾的文件\n",
    "    temp_list = os.listdir(path)\n",
    "    for file in temp_list:\n",
    "        temp_path = os.path.join(path, file)\n",
    "        if os.path.isdir(temp_path):\n",
    "            print_list_dir(doc, temp_path, suffix)\n",
    "        elif temp_path.endswith(suffix):\n",
    "            doc.append(temp_path)\n",
    "\n",
    "if use_dataset == 1:\n",
    "    sub_file_dir = os.listdir(file_dir)       \n",
    "    for sub_dir in sub_file_dir:\n",
    "        root_path = os.path.join(file_dir, sub_dir)\n",
    "        file_temp = os.listdir(root_path)\n",
    "        file_temp = [os.path.join(root_path, file_name) for file_name in file_temp]\n",
    "        file_list.extend(file_temp)\n",
    "        \n",
    "    file_list.sort()\n",
    "    random.shuffle(file_list)\n",
    "    \n",
    "    train_list = file_list[0:100000]\n",
    "    test_list = file_list[100000: 110000]\n",
    "    val_list = file_list[110000:]\n",
    "elif use_dataset == 2:\n",
    "    bg_gaus_datalist = []\n",
    "    rand_gaus_1s_datalist = []\n",
    "    file_gs_dir = 'guassian-bg_only/'\n",
    "    file_1s_dir = 'guassian-rand_1s/'\n",
    "    \n",
    "    with open(\"dataset_gaussian_bg_only.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            data_path_pre = line.strip('\\n')\n",
    "            data_path = data_path_pre.replace(\"bg_guassian\", \"guassian-bg\")\n",
    "            bg_gaus_datalist.append(os.path.join(file_dir, file_gs_dir, data_path))\n",
    "    \n",
    "    with open(\"dataset_gaussian_rand_1s.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            data_path = line.strip('\\n')\n",
    "            rand_gaus_1s_datalist.append(os.path.join(file_dir, file_1s_dir, data_path))\n",
    "            \n",
    "elif use_dataset == 3:    \n",
    "    bg_only_list = []\n",
    "    path = \"/data/gwx/musan/Version_2_speech_artificial_8s_noise_dataset/guassian-bg_only\"\n",
    "    print_list_dir(bg_only_list, path)\n",
    "\n",
    "    rand_1s_list = []\n",
    "    path = \"/data/gwx/musan/Version_2_speech_artificial_8s_noise_dataset/guassian-rand_1s\"\n",
    "    print_list_dir(rand_1s_list, path)\n",
    "            \n",
    "#     file_list = bg_gaus_datalist + rand_gaus_1s_datalist\n",
    "    file_list = bg_only_list + rand_1s_list\n",
    "    file_list.sort()\n",
    "    random.shuffle(file_list)\n",
    "    \n",
    "    train_list = file_list[0:160000]\n",
    "    test_list = file_list[160000: 175000]\n",
    "    val_list = file_list[175000:]\n",
    "\n",
    "train_list.sort()\n",
    "test_list.sort()\n",
    "val_list.sort()\n",
    "# selcet_list = []\n",
    "# for file in file_list:\n",
    "#     file_size = os.path.getsize(os.path.join(file_dir, file))\n",
    "#     if file_size > 1000000:\n",
    "#         selcet_list.append(file)\n",
    "#         print(file)\n",
    "# selcet_list.sort()\n",
    "print(len(file_list))\n",
    "print(file_list[:2])\n",
    "            \n",
    "# train_dataset = speech_Dataset(file_dir, train_list, evalu_target)\n",
    "train_dataset = speech_Dataset('', train_list, evalu_target, wav_path, ref_npy_path)\n",
    "train_loader = DataLoader(train_dataset,batch_size=1, num_workers=16, shuffle=True)\n",
    "\n",
    "# val_dataset = speech_Dataset(file_dir, val_list, evalu_target)\n",
    "val_dataset = speech_Dataset('', val_list, evalu_target, wav_path, ref_npy_path)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, num_workers=8, shuffle=False)\n",
    "\n",
    "# test_dataset = speech_Dataset(file_dir, test_list, evalu_target)\n",
    "test_dataset = speech_Dataset('', test_list, evalu_target, wav_path, ref_npy_path)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encoder_layer_1 = nn.TransformerEncoderLayer(d_model=64, nhead=1, batch_first=True, dim_feedforward=128)\n",
    "# model = nn.TransformerEncoder(encoder_layer_1, num_layers=2)\n",
    "# model = CNN_ATT_FC()\n",
    "\n",
    "if select_model is 'cnn':\n",
    "    model = Ushape_Att_Backbone(input_channels=1, out_channels=4, Ushape_layers=model_layer, att_dims=neurons_num)\n",
    "elif select_model is 'mosnet':    \n",
    "    model = CNN_BLSTM()\n",
    "elif select_model is 'stoinet':    \n",
    "    model = Stoi_net()\n",
    "elif select_model is 'dense_cnn_with_ave': \n",
    "    model = Dense_Ushape_CNN_Backbone_With_Ave(input_channels=1, out_channels=4, Ushape_layers=model_layer, att_dims=neurons_num)\n",
    "elif select_model is 'blstm_with_ave': \n",
    "    model = Blstm_With_Ave(input_channels=1, out_channels=4, Ushape_layers=model_layer, att_dims=neurons_num)\n",
    "\n",
    "# pe = PositionalEncoding(64, 0, 10)\n",
    "#output = model(src, n)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, verbose=True)\n",
    "# loss_fcn = nn.MSELoss()\n",
    "loss_fcn1 = nn.MSELoss()\n",
    "loss_fcn2 = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 4.652361\n"
     ]
    }
   ],
   "source": [
    "min_train_loss = 0.02 * 3 * 2\n",
    "min_val_loss = 0.03 * 3 * 2\n",
    "\n",
    "for iter_epoch in range(num_epoch):\n",
    "    model.train()\n",
    "#     loss_np = np.zeros(0)\n",
    "    loss_np_500 = np.zeros(0)\n",
    "    for num, data in enumerate(train_loader):\n",
    "#         frame_score = data[0].unsqueeze(1).type(torch.float32)\n",
    "#         frame_score = pe(frame_score)\n",
    "#         print(frame_score[0,:,:])\n",
    "#         frame_score, indices = torch.sort(frame_score)\n",
    "        system_score = data[1].type(torch.float32)\n",
    "#         print(system_score, type(system_score))\n",
    "#         print(system_score < 4)\n",
    "        if system_score == 0:\n",
    "            print('pass 0')\n",
    "            continue\n",
    "        \n",
    "        frame_feature = data[2].type(torch.float32)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "#             frame_score = frame_score.cuda()\n",
    "            system_score = system_score.cuda()\n",
    "            frame_feature = frame_feature.cuda()\n",
    "        \n",
    "#         output = model(frame_feature)\n",
    "        avg_score, frame_score = model(frame_feature)\n",
    "\n",
    "#         loss = loss_fcn(output, system_score)\n",
    "        loss = loss_fcn1(system_score, avg_score) + loss_fcn2(system_score, frame_score)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         loss_np = np.append(loss_np, loss.cpu().detach().item())\n",
    "        loss_np_500 = np.append(loss_np_500, loss.cpu().detach().item())\n",
    "        \n",
    "    \n",
    "            \n",
    "#         print(loss.item())\n",
    "        if num % 500 == 0:\n",
    "            if num // 500 == 0:\n",
    "                mean_loss = float(\"inf\") \n",
    "                logger.info(\"iter: %d, loss: %f\"%(num, loss.item()))\n",
    "            \n",
    "            if num // 500 >= 1:\n",
    "#                 mean_loss = loss_np[(num//500-1)*500:num//500*500].mean()\n",
    "                mean_loss = loss_np_500.mean()\n",
    "                logger.info(\"iter: %d, loss: %f, mean loss: %f\"%(num, loss.item(), mean_loss))\n",
    "                loss_np_500 = np.zeros(0)\n",
    "                \n",
    "            if mean_loss < min_train_loss:\n",
    "                model.eval()\n",
    "                val_loss_list = np.zeros(0)\n",
    "                for val_num, val_data in enumerate(val_loader):\n",
    "                    val_system_score = val_data[1].type(torch.float32)\n",
    "                    if val_system_score == 0:\n",
    "                        print('pass 0')\n",
    "                        continue\n",
    "\n",
    "                    val_frame = val_data[2].type(torch.float32)\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        val_system_score = val_system_score.cuda()\n",
    "                        val_frame = val_frame.cuda()\n",
    "\n",
    "#                         val_output = model(val_frame)\n",
    "                    val_avg_score, val_frame_score = model(val_frame)\n",
    "\n",
    "#                         val_loss = loss_fcn(val_output, val_system_score)\n",
    "                    val_loss = loss_fcn1(val_system_score, val_avg_score) + loss_fcn2(val_system_score, val_frame_score)\n",
    "\n",
    "\n",
    "                    val_loss_list = np.append(val_loss_list, val_loss.cpu().detach().item())\n",
    "\n",
    "                if val_loss_list.mean() < min_val_loss:\n",
    "                    min_train_loss = mean_loss.copy()\n",
    "                    min_val_loss = val_loss_list.mean().copy()\n",
    "                    torch.save(model.state_dict(), model_ckpt_save_path)\n",
    "                logger.info(\"val in iter %d, epoch %d, mean loss is %.8f, min_val_loss is %.8f, min_train_loss is %.8f\"\n",
    "                            %(num, iter_epoch, val_loss_list.mean(), min_val_loss, min_train_loss))\n",
    "                model.train()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_list.mean()\n",
    "# MSE = np.square(pred_list-gt_list).mean()\n",
    "# print(loss_list.mean(), MSE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred_list = np.zeros(0)\n",
    "gt_list = np.zeros(0)\n",
    "loss_list = np.zeros(0)\n",
    "model.load_state_dict(torch.load(model_ckpt_save_path))\n",
    "\n",
    "for num, data in enumerate(test_loader):\n",
    "#         frame_score = data[0].unsqueeze(1).type(torch.float32)\n",
    "#         frame_score = pe(frame_score)\n",
    "#         print(frame_score[0,:,:])\n",
    "#         frame_score, indices = torch.sort(frame_score)\n",
    "        system_score = data[1].type(torch.float32)\n",
    "        if system_score == 0:\n",
    "            print('pass 0')\n",
    "            continue\n",
    "        \n",
    "        frame_feature = data[2].type(torch.float32)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "#             frame_score = frame_score.cuda()\n",
    "            system_score = system_score.cuda()\n",
    "            frame_feature = frame_feature.cuda()\n",
    "        \n",
    "#         output = model(frame_feature)\n",
    "        avg_score, frame_score = model(frame_feature)\n",
    "\n",
    "        loss = loss_fcn1(frame_score, system_score)\n",
    "#         loss = loss_fcn1(system_score, avg_score) + loss_fcn2(system_score, frame_score)\n",
    "\n",
    "#         pred = output.cpu().detach().numpy()[0, 0]\n",
    "        pred = avg_score.cpu().detach().numpy()[0, 0]\n",
    "    \n",
    "        gt = system_score.cpu().detach().numpy()[0]\n",
    "        loss = loss.cpu().item()\n",
    "        \n",
    "        pred_list = np.append(pred_list, pred)\n",
    "        gt_list = np.append(gt_list, gt)\n",
    "        loss_list = np.append(loss_list, loss)\n",
    "        logger.info(\"iter: %d, pred: %f, gt: %f, system loss: %f\"%(num, pred, gt, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(log_model_save_path, \"prediction of testset\"), pred_list)\n",
    "np.save(os.path.join(log_model_save_path, \"GT of testset\"), gt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_x = [0, 1, 2, 3, 4, 5, 6]#预测得分\n",
    "# gt_y = [0, 1, 2, 3, 4, 5, 6]#真实得分\n",
    "fig1 = plt.figure()\n",
    "title_name = \"Pred_GT-Relationship\"\n",
    "a1 = fig1.add_axes([0,0,1,1])\n",
    "\n",
    "plt.title(title_name)\n",
    "plt.grid(color = 'b', linestyle = '--', linewidth = 0.1)\n",
    "plt.scatter(pred_list,gt_list, alpha=0.5)#预测得分和真实得分的关系图\n",
    "\n",
    "a1.set_ylim(0,5)\n",
    "a1.set_xlim(0,5)\n",
    "\n",
    "plt.plot()\n",
    "plt.show()\n",
    "\n",
    "fig1.savefig(os.path.join(log_model_save_path, title_name+'.png'), dpi = 350, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_name = 'Testset-GT_score-Distribution'\n",
    "fig2 = plt.figure()\n",
    "plt.hist(gt_list,\n",
    "        bins = 50,\n",
    "        color = 'steelblue',\n",
    "        edgecolor = 'black'\n",
    "        )\n",
    "plt.xlabel(title_name)\n",
    "plt.ylabel('number')\n",
    "plt.show()\n",
    "fig2.savefig(os.path.join(log_model_save_path, title_name+'.png'), dpi = 350, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title_name = 'Testset-Prediction_score-Distribution'\n",
    "fig3 = plt.figure()\n",
    "plt.hist(pred_list,\n",
    "        bins = 50,\n",
    "        color = 'steelblue',\n",
    "        edgecolor = 'black'\n",
    "        )\n",
    "plt.xlabel('Pred score')\n",
    "plt.ylabel('number')\n",
    "plt.show()\n",
    "fig3.savefig(os.path.join(log_model_save_path, title_name+'.png'), dpi = 350, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "MSE = np.square(pred_list-gt_list).mean()\n",
    "print(\"MSE: \", MSE)\n",
    "\n",
    "LCC = pearsonr(pred_list, gt_list)\n",
    "print(\"LCC: \", LCC[0])\n",
    "\n",
    "SRCC = spearmanr(pred_list, gt_list)\n",
    "print(\"SRCC: \", SRCC[0])\n",
    "\n",
    "indicators_name = 'result.txt'\n",
    "with open(os.path.join(log_model_save_path, indicators_name), 'w') as writer:\n",
    "    writer.write(\"MSE: %f\\n\" %MSE)\n",
    "    writer.write(\"LCC: %f\\n\" %LCC[0])\n",
    "    writer.write(\"SRCC: %f\\n\" %SRCC[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
